{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirm\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entropy(dist, samples = False):\n",
    "    \"\"\"\n",
    "        calculates entropy and joint entropy. If samples = True, returns torch array for each distribution sample.\n",
    "    \"\"\"\n",
    "    dist = torch.Tensor(dist)\n",
    "\n",
    "    # if distribution has 0 values, add small value to avoid log(0)\n",
    "    dist += 1e-3\n",
    "\n",
    "    if samples == False:\n",
    "\n",
    "        if len(dist.shape) == 1:\n",
    "        # expected value is just a dot product\n",
    "            return -torch.dot(dist,torch.log2(dist))\n",
    "        \n",
    "\n",
    "        # if joint entropy is to be calculated\n",
    "        else:\n",
    "            n_variables = len(dist.shape)\n",
    "\n",
    "            # dimensions of variables to reduce\n",
    "            dims = tuple([i for i in range(n_variables)])\n",
    "\n",
    "            return -(dist*torch.log2(dist)).sum(dim = dims)\n",
    "    \n",
    "\n",
    "    # assumes first dimension is samples\n",
    "    else:\n",
    "        n_variables = len(dist.shape)-1\n",
    "\n",
    "        # dimensions of variables to reduce\n",
    "        dims = tuple([i+1 for i in range(n_variables)])\n",
    "\n",
    "\n",
    "        return -(dist*torch.log2(dist)).sum(dim = dims)\n",
    "\n",
    "\n",
    "\n",
    "def renyi_entropy(dist, r = 1, samples = False):\n",
    "    \"\"\"calculates renyi entropy. Similar to DIT\"\"\"    \n",
    "    dist = torch.Tensor(dist)\n",
    "    \n",
    "    \n",
    "    if r < 0:\n",
    "        msg = \"`order` must be a non-negative real number\"\n",
    "        raise ValueError(msg)\n",
    "    \n",
    "    if r == 1:\n",
    "        return entropy(dist, samples=samples)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # if distribution has 0 values, add small value to avoid log(0)\n",
    "        dist += 1e-3\n",
    "\n",
    "\n",
    "        # if one dimensional distribution\n",
    "        if len(dist.shape) == 1:\n",
    "            return (1/(1-r))*torch.log2(torch.sum(dist**r))\n",
    "\n",
    "\n",
    "        # assumes first dimension is for the samples\n",
    "        elif samples:\n",
    "            n_variables = len(dist.shape)\n",
    "\n",
    "            # dimensions of variables to reduce\n",
    "            dims = tuple([i+1 for i in range(n_variables)])\n",
    "            \n",
    "            return (1/(1-r))*torch.log2(torch.sum(dist**r, dim=dims))\n",
    "        \n",
    "\n",
    "        # joint renyi entropy\n",
    "        else:\n",
    "            n_variables = len(dist.shape)\n",
    "\n",
    "            # dimensions of variables to reduce\n",
    "            dims = tuple([i for i in range(n_variables)])\n",
    "            \n",
    "            return (1/(1-r))*torch.log2(torch.sum(dist**r, dim=dims))\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def calculate__independent_joint(dist1, dist2):\n",
    "    \"\"\" (n_samples, m), (n_samples, n) -> (n_samples, m,n)\"\"\"\n",
    "    \n",
    "    # squeeze distributions\n",
    "    dist1 = dist1.squeeze()\n",
    "    dist2 = dist2.squeeze()\n",
    "\n",
    "    # adds a dimension for samples when only one sample is available\n",
    "    if len(dist1.shape) == 1:\n",
    "        dist1 = dist1.unsqueeze(0)\n",
    "\n",
    "    if len(dist2.shape) == 1:\n",
    "        dist2 = dist2.unsqueeze(0)\n",
    "\n",
    "    # return joint probabilities\n",
    "    return torch.bmm(dist1.unsqueeze(-1), dist2.unsqueeze(1))\n",
    "\n",
    "\n",
    "# given joint distribution, calculate conditional\n",
    "def calculate_conditional(dist, indices):\n",
    "    \n",
    "    # marginal distributions where certain dimensions are 1.\n",
    "    marginals = torch.sum(dist, dim = indices, keepdim = True)\n",
    "\n",
    "    # expand marginals to have similar shape as before\n",
    "    marginals = marginals.expand(dist.shape)\n",
    "\n",
    "    return dist/marginals\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def common_entropy_oracle(x_y_distribution = torch.Tensor(), n_observed_states = (10,10), n_samples = 1, beta_values = np.linspace(0,0.1,50), iterations = 100):\n",
    "    \"\"\"find common entropy of distributions\n",
    "        input is torch tensor of size n_samples*n*m \n",
    "    \"\"\"\n",
    "\n",
    "    latent_states = n_observed_states[0]\n",
    "\n",
    "    # sample observed distributions. this is working properly.\n",
    "    sampled_observed = x_y_distribution\n",
    "\n",
    "\n",
    "    cmi_threshold = 0.001\n",
    "\n",
    "\n",
    "\n",
    "    data = Data(n_observed_states=n_observed_states, \n",
    "                latent_entropy_threshold=1, model_type='latent')\n",
    "\n",
    "\n",
    "    # create a null list of observed samples\n",
    "    final_joints = torch.zeros(size = (n_samples, ) + n_observed_states + (latent_states,))\n",
    "\n",
    "\n",
    "    z_entropies = 5*np.ones(n_samples)\n",
    "\n",
    "    for beta in beta_values:\n",
    "        latentsearch = LatentSearch(beta = beta, iterations = iterations)\n",
    "\n",
    "        # initialize q(z|x,y)\n",
    "        initializations = data.initialize_joint_conditional_distributions(n_samples=n_samples)\n",
    "\n",
    "\n",
    "        joint = latentsearch.fit(observed_joint=sampled_observed, initialization=initializations)\n",
    "\n",
    "\n",
    "        YZ = joint.sum(dim=1)\n",
    "        XZ = joint.sum(dim=2)\n",
    "\n",
    "        # calculate cmi\n",
    "        cmi_ = cmi(XZ = XZ, YZ = YZ, XYZ = joint, Z = joint.sum(dim=(1,2)), samples = True)\n",
    "        \n",
    "\n",
    "        z = joint.sum(dim=(1,2))\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # check if cmi constraint is satisfied.\n",
    "            if cmi_[i] <= cmi_threshold:\n",
    "                \n",
    "                z_entropy = entropy(z[i,:],samples=True)\n",
    "                \n",
    "                # if minimum, keep it.\n",
    "                if z_entropy<z_entropies[i]:\n",
    "                    final_joints[i,:,:,:] = joint[i,:,:,:]\n",
    "                    z_entropies[i] = z_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #if some entropy never changed, its cmi was not found\n",
    "    if z_entropies.max()>4:\n",
    "        print(\"CMI condition was never satisfied\")\n",
    "        return ValueError\n",
    "\n",
    "\n",
    "\n",
    "    # returns entropy of z\n",
    "    return z_entropies\n",
    "\n",
    "\n",
    "\n",
    "# returns wrong sized distribution. still using it for cmi because it's default\n",
    "def calc_stats(data, var_size, weights=None):\n",
    "    \"\"\"\n",
    "    Calculate the counts of instances in the data\n",
    "    :param data: a dataset of categorical features\n",
    "    :param var_size: a vector defining the cardinalities of the features\n",
    "    :param weights: a vector of non-negative weights for data samples.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sz_cum_prod = [1]\n",
    "    for node_i in range(len(var_size) - 1):\n",
    "        sz_cum_prod += [sz_cum_prod[-1] * var_size[node_i]]\n",
    "\n",
    "    sz_prod = sz_cum_prod[-1] * var_size[-1]\n",
    "\n",
    "    data_idx = np.dot(data, sz_cum_prod)\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        # hist_count, _ = np.histogram(data_idx, np.arange(sz_prod + 1), weights=weights)\n",
    "        hist_count = np.bincount(data_idx, minlength=sz_prod, weights=weights)\n",
    "    except MemoryError as error:\n",
    "        print('Out of memory')\n",
    "        return None\n",
    "    return hist_count\n",
    "\n",
    "\n",
    "\n",
    "# calculate distribution from categorical data\n",
    "def calc_joint_dist(data = np.array([]), states = (10,10)):\n",
    "    \"\"\"calculates empirical pdf of data with discrete states\n",
    "\n",
    "            data = np.array(np.random.choice(3,(5,3)), dtype='int32')\n",
    "            calc_joint_dist(data, states = (3,3,3))\n",
    "            \n",
    "    \"\"\"\n",
    "    joint_dist, _ = np.histogramdd(data, bins = states)\n",
    "    joint_dist /= joint_dist.sum()\n",
    "\n",
    "    return joint_dist\n",
    "\n",
    "\n",
    "\n",
    "def cmi(XZ, YZ, XYZ, Z, samples = True):\n",
    "    \"\"\"Calculates the conditional mutual information (CMI) I(X;Y|Z).\"\"\"\n",
    "\n",
    "    # Calculate joint and marginal entropies\n",
    "    H_XYZ = entropy(XYZ, samples = samples)\n",
    "    H_XZ = entropy(XZ, samples = samples)\n",
    "    H_Z = entropy(Z, samples = samples)\n",
    "    H_YZ = entropy(YZ, samples = samples)\n",
    "\n",
    "    # Calculate CMI\n",
    "    return H_XZ + H_YZ - H_XYZ - H_Z\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Search Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for distribution generation\n",
    "class Data():\n",
    "    \"\"\"Samples distributions from a given latent graph\"\"\"\n",
    "\n",
    "    def __init__(self, n_observed_states = (10, 10), latent_entropy_threshold = 1, model_type = 'latent'):\n",
    "\n",
    "        self.n_latent_states = max(n_observed_states)\n",
    "        self.n_observed_states = n_observed_states\n",
    "        self.model_type = model_type\n",
    "        self.latent_entropy_treshold = latent_entropy_threshold\n",
    "\n",
    "\n",
    "    # sample from the latent distributions\n",
    "    def sample_latent(self, n_samples):\n",
    "        \n",
    "        # sampling low entropy latent distributions\n",
    "        iterations = 10\n",
    "\n",
    "        N_samples_factor = 10\n",
    "\n",
    "        low_entropy_samples = None\n",
    "\n",
    "        for i in range(iterations):\n",
    "\n",
    "            # latent distribution\n",
    "            alpha = torch.Tensor(self.n_latent_states*[1/(2**i)])\n",
    "            latent_distribution = torch.distributions.Dirichlet(alpha)\n",
    "\n",
    "\n",
    "            # sample distribution 10N times\n",
    "            latent_samples = latent_distribution.sample(sample_shape = (N_samples_factor*n_samples,))\n",
    "\n",
    "            # calculate entropy for all\n",
    "            latent_samples_entropy = renyi_entropy(latent_samples, r=1, samples=True)\n",
    "\n",
    "\n",
    "            # find low entropy samples\n",
    "            low_entropy_samples = latent_samples[latent_samples_entropy<=self.latent_entropy_treshold, :]\n",
    "\n",
    "            # if count >= N, break\n",
    "            if low_entropy_samples.shape[0] >= n_samples:\n",
    "                return low_entropy_samples[0:n_samples,:]\n",
    "\n",
    "\n",
    "        # if not found, run error\n",
    "        raise Exception(\"did not find enough low entropy samples\")\n",
    "\n",
    "\n",
    "\n",
    "    # sample conditional using method in paper. returns in shape (n,x,y,z)\n",
    "    # assumes x, y are independent\n",
    "    def sample_conditional(self, n_samples = 10):\n",
    "\n",
    "        if self.model_type == 'latent':\n",
    "            \n",
    "            # sample conditionals independently\n",
    "            alpha_x = torch.Tensor(self.n_observed_states[0]*[1])\n",
    "            alpha_y = torch.Tensor(self.n_observed_states[1]*[1])\n",
    "            x_z_distribution = torch.distributions.Dirichlet(alpha_x)\n",
    "            y_z_distribution = torch.distributions.Dirichlet(alpha_y)\n",
    "\n",
    "            # P(X|Z). has shape n_samples, observed_states, latent_states\n",
    "            x_z_samples = x_z_distribution.sample(sample_shape = (n_samples, self.n_latent_states,))\n",
    "            x_z_samples = torch.transpose(x_z_samples, 1,2)\n",
    "\n",
    "            # P(Y|Z)\n",
    "            y_z_samples = y_z_distribution.sample(sample_shape = (n_samples, self.n_latent_states,))\n",
    "            y_z_samples = torch.transpose(y_z_samples, 1,2)\n",
    "\n",
    "            # P(X,Y|Z). \n",
    "            x_z_samples = x_z_samples.unsqueeze(2)\n",
    "            y_z_samples = y_z_samples.unsqueeze(1)\n",
    "            P_x_y_given_z = x_z_samples * y_z_samples \n",
    "\n",
    "            return P_x_y_given_z\n",
    "\n",
    "\n",
    "\n",
    "    # sample P(x,y)\n",
    "    def sample_observed(self, n_samples = 10):\n",
    "\n",
    "        # P(X,Y,Z) = P(X,Y|Z)*P(Z)\n",
    "        # has shape (n,X,Y,Z)\n",
    "        x_y_condition_z = self.sample_conditional(n_samples=n_samples)\n",
    "\n",
    "        # has shape (n,Z)\n",
    "        z = self.sample_latent(n_samples = n_samples)\n",
    "\n",
    "        # dimensions: n,x,y,z\n",
    "        x_y_z = x_y_condition_z*z.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # marginalize over z dimension\n",
    "        return torch.sum(x_y_z, dim=3), z\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def initialize_joint_conditional_distributions(self, n_samples = 10):\n",
    "        \"\"\" initialization for latentsearch. returns shape (n, x, y, z)\"\"\"\n",
    "        alpha = torch.Tensor(self.n_latent_states*[1])\n",
    "\n",
    "        distribution = torch.distributions.Dirichlet(alpha)\n",
    "\n",
    "        # note that this has shape n, x, y, z still.\n",
    "        latent_conditional_samples = distribution.sample(sample_shape = (n_samples, self.n_observed_states[0], self.n_observed_states[1]))\n",
    "\n",
    "        return latent_conditional_samples\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causality-Lab Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sample from the latent distributions\n",
    "def sample_latent_distributions(n_latent_states, latent_entropy_threshold, n_samples):\n",
    "    \"\"\"returns n_samples low_entropy samples from a Dirichlet distribution\"\"\"\n",
    "    # sampling low entropy latent distributions\n",
    "    iterations = 10\n",
    "\n",
    "    N_samples_factor = 10\n",
    "\n",
    "    low_entropy_samples = None\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # latent distribution\n",
    "        alpha = torch.Tensor(n_latent_states*[1/(2**i)])\n",
    "        latent_distribution = torch.distributions.Dirichlet(alpha)\n",
    "\n",
    "\n",
    "        # sample distribution 10N times\n",
    "        latent_samples = latent_distribution.sample(sample_shape = (N_samples_factor*n_samples,))\n",
    "\n",
    "        # calculate entropy for all\n",
    "        latent_samples_entropy = renyi_entropy(latent_samples, r=1, samples=True)\n",
    "\n",
    "        # find low entropy samples\n",
    "        low_entropy_samples = latent_samples[latent_samples_entropy<=latent_entropy_threshold, :]\n",
    "\n",
    "\n",
    "        # if count >= N, break\n",
    "        if low_entropy_samples.shape[0] >= n_samples:\n",
    "            return low_entropy_samples[0:n_samples,:]\n",
    "\n",
    "\n",
    "    # if not found, run error\n",
    "    raise Exception(\"did not find low entropy samples\")\n",
    "\n",
    "\n",
    "\n",
    "# generate graph\n",
    "# similar to causality-lab\n",
    "def sample_data_from_dag_dirichlet(in_dag, num_samples, states):\n",
    "    \"\"\"\n",
    "    :param num_samples: number of samples (dataset records)\n",
    "    :param states: number of possible states for each variable in the dag\n",
    "    :return: Sampled dataset in the form of a 2D NumPy array\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # start with empty node. since we're running through the topological order, no problem should ocurr.\n",
    "    data = np.empty((num_samples, len(in_dag.nodes_set)))\n",
    "\n",
    "\n",
    "    topological_order = in_dag.find_topological_order()\n",
    "\n",
    "\n",
    "    \n",
    "    for node in topological_order:\n",
    "\n",
    "\n",
    "        # find parents\n",
    "        parents_set = in_dag.parents(node)\n",
    "\n",
    "\n",
    "        # if no parent, use low-entropy sampling\n",
    "        if len(parents_set) == 0:\n",
    "\n",
    "            # find a distribution. cast np array because it returns torch\n",
    "            distributions = np.repeat(np.array(sample_latent_distributions(states[node], latent_entropy_threshold=1, n_samples=1)), num_samples, axis =0)\n",
    "\n",
    "        # use Chickering and Meek Method for sampling with dependencies\n",
    "        else:\n",
    "\n",
    "            # find identifiers for each sample\n",
    "            current_identifiers = find_indentifiers(data, states, parents_set, num_samples)\n",
    "\n",
    "            # sample dirichlet respecting parent set\n",
    "            # construct mean vector: \n",
    "            mean_vector = np.array([1/(i+1) for i in range(states[node])])\n",
    "            \n",
    "            # cyclic shift\n",
    "            # sample dirichlet with mean vector\n",
    "            mean_vectors = [np.roll(mean_vector, current_identifiers[i]) for i in range(num_samples)]\n",
    "            \n",
    "            # sample dirichlet for mean_vectors[i]\n",
    "            distributions = [np.random.dirichlet(mean_vectors[i]) for i in range(num_samples)]\n",
    "            distributions = np.array(distributions).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # sample from distributions\n",
    "        data[:,node] = vectorized_sampling(distributions)\n",
    "\n",
    "\n",
    "\n",
    "    return data.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_indentifiers(data, states, parents_set, num_samples):\n",
    "    \"\"\"Finds identifiers of the distribution conditioned on the parent set. \n",
    "    return shape: (n_samples,)\"\"\"\n",
    "\n",
    "    # Generate possible identifiers based on the states of the parents\n",
    "    total_configurations = np.prod([states[parent] for parent in parents_set])\n",
    "\n",
    "    # shape (n, m, n, k) for parents with possible states m, n, and k. For better access to value of identifier.\n",
    "    identifiers = np.arange(total_configurations).reshape((1,) + tuple(states[parent] for parent in parents_set))\n",
    "    identifiers = np.repeat(identifiers, repeats=num_samples, axis=0)\n",
    "\n",
    "\n",
    "    parent_values = np.stack([data[:, parent] for parent in parents_set], axis=1)\n",
    "\n",
    "\n",
    "    samples = np.arange(num_samples)\n",
    "\n",
    "\n",
    "    # shape: (num_samples, 1 + len(parents_set))\n",
    "    indices = np.column_stack((samples, parent_values)).astype(int)\n",
    "\n",
    "\n",
    "    current_identifiers = identifiers[tuple(indices.T)]\n",
    "    \n",
    "    return current_identifiers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vectorized_sampling(distributions):\n",
    "    \"\"\"returns samples from given categorical distributions in form (n_samples, n_states)\n",
    "    source: https://stackoverflow.com/questions/47722005/vectorizing-numpy-random-choice-for-given-2d-array-of-probabilities-along-an-a\"\"\"\n",
    "    \n",
    "    n_samples = distributions.shape[0]\n",
    "\n",
    "    # returns n samples from uniform distribution between 0 and 1\n",
    "    r = np.random.rand(n_samples)\n",
    "    \n",
    "    # cumsum(1): cdf\n",
    "    # argmax: first place when we're above the cdf\n",
    "    return (distributions.cumsum(1) > r[:,None]).argmax(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates joint (renyi_{1}-common entropy)\n",
    "\n",
    "class LatentSearch():\n",
    "    \n",
    "    def __init__(self, beta = 1, iterations = 100):\n",
    "        self.beta = beta\n",
    "        self.data_over_time = []\n",
    "        self.iterations = iterations\n",
    "\n",
    "\n",
    "    def fit(self, observed_joint = torch.Tensor([]), initialization = torch.Tensor([])):\n",
    "        \n",
    "        \"\"\"    \n",
    "        calculates q(z|x,y) from data. Best possible distribution of z given x and y, while keeping x and y's mutual information minimum\n",
    "\n",
    "        observed joint has shape (n, x, y). Initialization has shape (n, x, y, z)\"\"\"\n",
    "\n",
    "        # if not one sample, add one.\n",
    "        n_samples = observed_joint.shape[0]\n",
    "        latent_states = initialization.shape[3]\n",
    "        x_states = observed_joint.shape[1]\n",
    "        y_states = observed_joint.shape[2]\n",
    "\n",
    "\n",
    "        # q(z|x,y)\n",
    "        z_conditioned_x_y = initialization\n",
    "\n",
    "        # add dimension to observed. this is for expanding the last dimension\n",
    "        observed = observed_joint.unsqueeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "\n",
    "        # form joint.\n",
    "\n",
    "            # joint has shape (n,x,y,z)\n",
    "            joint = z_conditioned_x_y*observed.expand(-1, -1, -1, latent_states)\n",
    "\n",
    "            # calculates q(z|x), q(z|y), and q(z)\n",
    "            q_z_x, q_z_y, q_z = self.calculate(joint)\n",
    "\n",
    "            # numerical convergence\n",
    "            small_value = 1e-2\n",
    "            q_z_x += (q_z_x<small_value).float()*small_value\n",
    "            q_z_y += (q_z_y<small_value).float()*small_value\n",
    "            q_z += (q_z<small_value).float()*small_value\n",
    "\n",
    "            # updates q(z|x,y)\n",
    "            z_conditioned_x_y = self.update(q_z_x, q_z_y, q_z)\n",
    "\n",
    "            # save update?\n",
    "\n",
    "        small_value = 1e-4\n",
    "        z_conditioned_x_y += (z_conditioned_x_y).float()*small_value\n",
    "\n",
    "\n",
    "        # joint has shape (n,x,y,z)\n",
    "        joint = z_conditioned_x_y*observed.expand(-1, -1, -1, latent_states) \n",
    "\n",
    "    \n",
    "        return joint\n",
    "\n",
    "        \n",
    "    # joint has shape (n,x,y,z)\n",
    "    def calculate(self, joint):\n",
    "\n",
    "        # q(z|x). has shape (n, x, 1, z)\n",
    "        q_z_x = torch.sum(joint, dim=2, keepdim=True)/torch.sum(joint, dim = (2,3), keepdim=True)\n",
    "\n",
    "        # q(z|y). has shape (n, 1, y, z)\n",
    "        q_z_y = torch.sum(joint, dim=1, keepdim=True)/torch.sum(joint, dim = (1,3), keepdim=True)\n",
    "\n",
    "        # q(z). has shape (n, 1, 1, z)\n",
    "        q_z = torch.sum(joint, dim=(1,2), keepdim=True)\n",
    "\n",
    "\n",
    "        return q_z_x, q_z_y, q_z\n",
    "\n",
    "\n",
    "\n",
    "    # update joint distribution\n",
    "    def update(self, q_z_x, q_z_y, q_z):\n",
    "\n",
    "\n",
    "        x_states = q_z_x.shape[1]\n",
    "        y_states = q_z_y.shape[2]\n",
    "        z_states = q_z.shape[3]\n",
    "\n",
    "        # q(z|x)q(z|y). has shape (n, x, y, z)\n",
    "        numerator = q_z_x * q_z_y\n",
    "        \n",
    "        # # add term to denominator to avoid division by zero. has shape (n, 1, 1, z)\n",
    "        denominator = (q_z**(1 - self.beta))\n",
    "        # denominator = (q_z**(1 - self.beta))\n",
    "\n",
    "        # has shape (n, x, y, z)\n",
    "        update = numerator/denominator\n",
    "    \n",
    "\n",
    "        # has shape (n, x, y, 1)\n",
    "        normalization = torch.sum(update, dim = 3, keepdim=True)\n",
    "\n",
    "\n",
    "        # returns q(z|x,y)\n",
    "        return (1/normalization) * update\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EntropicPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain, tee\n",
    "from causality_lab.causal_discovery_utils.constraint_based import LearnStructBase, unique_element_iterator\n",
    "from causality_lab.graphical_models import PDAG\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LearnStructPC(LearnStructBase):\n",
    "    \"\"\"Entropic Version of the Causality-lab implementation of the PC algorithm\"\"\"\n",
    "\n",
    "    def __init__(self, nodes_set, ci_test, entropic = False, F = False, data = None, states = None):\n",
    "        super().__init__(PDAG, nodes_set=nodes_set, ci_test=ci_test)\n",
    "        self.graph.create_complete_graph(nodes_set)  # Create a fully connected graph\n",
    "        self.overwrite_starting_graph = True  # if True, the sequence at which the CIs are tested affects the result\n",
    "        \n",
    "        # maysep\n",
    "        self.maysep = np.ones((len(nodes_set), len(nodes_set)))\n",
    "        \n",
    "        self.entropic = entropic\n",
    "        self.F = F\n",
    "        \n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "        # find values, states\n",
    "        self.states = states\n",
    "\n",
    "        # find empirical distribution of data\n",
    "        self.distribution = calc_joint_dist(data, states=self.states)\n",
    "        \n",
    "\n",
    "        \n",
    "    def learn_structure(self):\n",
    "        \"\"\"\n",
    "        Learn a CPDAG (completed partially directed graph) using the PC algorithm\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.learn_skeleton()\n",
    "\n",
    "        self.orient_v_structures()\n",
    "        self.graph.convert_bidirected_to_undirected()  # treat bi-directed (spurious) as undirected\n",
    "\n",
    "        # meek rules\n",
    "        self.graph.maximally_orient_pattern([1, 2, 3])\n",
    "\n",
    "\n",
    "    def _exit_cond(self, order):\n",
    "        \"\"\"\n",
    "        Check if the max fan-in is lower or equal to the order (exit-cond. is met)\n",
    "        :param order: condition set size of the CI-test\n",
    "        :return: True if exit condition is met\n",
    "        \"\"\"\n",
    "        for node in self.graph.nodes_set:\n",
    "            if self.graph.fan_in(node) > order:  # if a node have a large enough number of parents, exit cond. is false\n",
    "                return False\n",
    "        else:\n",
    "            return True  # didn't find a node with a large enough number of parents for CI test, so exit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def learn_skeleton(self):\n",
    "        cond_indep = self.ci_test.cond_indep\n",
    "\n",
    "        if self.overwrite_starting_graph:\n",
    "            source_cpdag = self.graph  # Not a copy!!! thus, edge deletions affect consequent CI queries\n",
    "        else:\n",
    "            source_cpdag = self.graph.copy()  # slower, but removes the dependence on the sequence of CI testing\n",
    "\n",
    "        cond_set_size = 0\n",
    "        while not self._exit_cond(cond_set_size):\n",
    "            for node_i, node_j in combinations(source_cpdag.nodes_set, 2):\n",
    "                if not source_cpdag.is_connected(node_i, node_j):\n",
    "                    continue\n",
    "\n",
    "                # maysep condition\n",
    "                if not self.maysep[node_i][node_j]: \n",
    "                    continue\n",
    "\n",
    "                pot_parents_i = source_cpdag.undirected_neighbors(node_i) - {node_j}\n",
    "                pot_parents_j = source_cpdag.undirected_neighbors(node_j) - {node_i}\n",
    "                cond_sets_i = combinations(pot_parents_i, cond_set_size)\n",
    "                cond_sets_j = combinations(pot_parents_j, cond_set_size)\n",
    "                cond_sets = unique_element_iterator(  # unique of\n",
    "                    chain(cond_sets_i, cond_sets_j)  # neighbors of node_i OR neighbors of node_j\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "                for cond_set in cond_sets:\n",
    "                    if cond_indep(node_i, node_j, cond_set):\n",
    "\n",
    "                        # if using entropicPC algorithm\n",
    "                        if self.entropic:\n",
    "                            # put cond_sets into a set for index selection\n",
    "                            all_indices = list(np.arange(len(self.distribution.shape)))                        \n",
    "\n",
    "                            # calculate distributions\n",
    "                            cond_set_dist = np.sum(self.distribution, axis = tuple(set(all_indices) - set(cond_set)))\n",
    "                            nodes_dist = np.sum(self.distribution, axis = tuple(set(all_indices) - {node_i, node_j}))\n",
    "                            \n",
    "                            # joint entropy of the conditioning set\n",
    "                            if len(cond_set) == 0:\n",
    "                                conditional_set_joint_entropy = 0.1*min(entropy(np.sum(nodes_dist, axis=1)), entropy(np.sum(nodes_dist, axis = 0)))\n",
    "                                # print(\"conditional len is 0\", conditional_set_joint_entropy)\n",
    "                            else:\n",
    "                                conditional_set_joint_entropy = entropy(cond_set_dist)\n",
    "                                # print(\"conditional len is not 0\", conditional_set_joint_entropy)\n",
    "\n",
    "  \n",
    "                            \n",
    "                            \n",
    "                            # common entropy of nodes i and j\n",
    "                            # this won't work if you're dealing with more than 1 sample!!!!\n",
    "                            common_entropy = common_entropy_oracle(torch.Tensor(nodes_dist).unsqueeze(0), \n",
    "                                                                   n_observed_states = (self.states[node_i], self.states[node_j]), \n",
    "                                                                   n_samples = 1, beta_values = np.linspace(0,0.1,50), iterations = 500)\n",
    "                            common_entropy = np.array(common_entropy)[0]\n",
    "\n",
    "                            # if common entropy condition is satisfied\n",
    "                            if conditional_set_joint_entropy >= common_entropy:\n",
    "                                self.graph.delete_edge(node_i, node_j)  # remove directed/undirected edge\n",
    "                                self.sepset.set_sepset(node_i, node_j, cond_set)\n",
    "                                break  # stop searching for independence as we found one and updated the graph accordingly\n",
    "\n",
    "                            elif self.F == True: \n",
    "                                # set mayseps of both node directions to False\n",
    "\n",
    "                                self.maysep[node_j,node_i] = False\n",
    "                                self.maysep[node_i, node_j] = False\n",
    "\n",
    "\n",
    "                            # else if common_entropy(x,y) >= 0.8*min(entropy(x), entropy(y))\n",
    "                            elif common_entropy >= 0.8*min(entropy(np.sum(nodes_dist, axis=1)), entropy(np.sum(nodes_dist, axis = 0))):\n",
    "                                self.maysep[node_i, node_j] = False\n",
    "                                self.maysep[node_j, node_i] = False\n",
    "\n",
    "                        \n",
    "                        else:                                                        \n",
    "                            self.graph.delete_edge(node_i, node_j)  # remove directed/undirected edge\n",
    "                            self.sepset.set_sepset(node_i, node_j, cond_set)\n",
    "                            break  # stop searching for independence as we found one and updated the graph accordingly\n",
    "\n",
    "\n",
    "\n",
    "            cond_set_size += 1  # now go again over all the edges and try to remove using a condition set size +1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def orient_v_structures(self):\n",
    "        # ToDo: Move this function to the PDAG class\n",
    "        # create a copy of edges\n",
    "        pre_neighbors = dict()\n",
    "        for node in self.graph.nodes_set:\n",
    "            pre_neighbors[node] = self.graph.undirected_neighbors(node).copy()  # undirected neighbors pre graph changes\n",
    "\n",
    "        # check each node if it can serve as new collider for a disjoint neighbors\n",
    "        for node_z in self.graph.nodes_set:\n",
    "            # check undirected neighbors\n",
    "            xy_nodes = pre_neighbors[node_z]  # undirected neighbors\n",
    "            for node_x, node_y in combinations(xy_nodes, 2):\n",
    "                if self.graph.is_connected(node_x, node_y):\n",
    "                    continue  # skip this pair as they are connected\n",
    "                if node_z not in self.sepset.get_sepset(node_x, node_y):\n",
    "                    self.graph.orient_edge(source_node=node_x, target_node=node_z)  # orient X --> Z\n",
    "                    self.graph.orient_edge(source_node=node_y, target_node=node_z)  # orient Y --> Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "n_observed_states = (5, 5)\n",
    "latent_states = n_observed_states[0]\n",
    "latent_entropy_threshold = 1\n",
    "model_type = 'latent'\n",
    "\n",
    "data = Data(n_observed_states=n_observed_states, \n",
    "            latent_entropy_threshold=1, model_type='latent')\n",
    "\n",
    "\n",
    "# sample observed distributions. this is working properly.\n",
    "sampled_observed, z = data.sample_observed(n_samples=n_samples)\n",
    "\n",
    "\n",
    "\n",
    "# feed into latentsearch\n",
    "beta_values = np.linspace(0, 0.1, 50)\n",
    "\n",
    "joint_distributions = []\n",
    "cmi_threshold = 0.001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cmi_values = []\n",
    "\n",
    "# create a null list of observed samples\n",
    "final_joints = torch.zeros(size = (n_samples, ) + n_observed_states + (latent_states,))\n",
    "\n",
    "# upper bound for z entropies. This is used to find the minimum z entropy for each sample\n",
    "z_entropies = 5*np.ones(n_samples)\n",
    "\n",
    "\n",
    "for beta in beta_values:\n",
    "    latentsearch = LatentSearch(beta = beta, iterations = 500)\n",
    "\n",
    "    # initialize q(z|x,y)\n",
    "    initializations = data.initialize_joint_conditional_distributions(n_samples=n_samples)\n",
    "\n",
    "\n",
    "    joint = latentsearch.fit(observed_joint=sampled_observed, initialization=initializations)\n",
    "\n",
    "\n",
    "    YZ = joint.sum(dim=1)\n",
    "    XZ = joint.sum(dim=2)\n",
    "\n",
    "    # calculate cmi\n",
    "    cmi_ = cmi(XZ = XZ, YZ = YZ, XYZ = joint, Z = joint.sum(dim=(1,2)), samples = True)\n",
    "    \n",
    "\n",
    "    z = joint.sum(dim=(1,2))\n",
    "\n",
    "\n",
    "    # for each sample\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        # check if cmi constraint is satisfied.\n",
    "        if cmi_[i] <= cmi_threshold:\n",
    "            \n",
    "            # z_entropy = entropy(z[i,:],samples=True)\n",
    "            z_entropy = entropy(z[i,:], samples=True)\n",
    "            \n",
    "            # if minimum, keep it.\n",
    "            if z_entropy<z_entropies[i]:\n",
    "                final_joints[i,:,:,:] = joint[i,:,:,:]\n",
    "                z_entropies[i] = z_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#if some entropy never changed, its cmi was not found\n",
    "if z_entropies.max()>4:\n",
    "    print(\"CMI condition was never satisfied\")\n",
    "    print(z_entropies)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    }
   ],
   "source": [
    "print((z_entropies<1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EntropicPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dag\n",
    "\n",
    "from causality_lab.graphical_models import PDAG\n",
    "from causality_lab.causal_discovery_utils.cond_indep_tests import CondIndepCMI\n",
    "from causality_lab.causal_discovery_utils.performance_measures import structural_hamming_distance_cpdag as SHD\n",
    "\n",
    "nodes_set = set(range(3))\n",
    "dag = DAG(nodes_set)\n",
    "\n",
    "pdag_copy = PDAG(nodes_set)\n",
    "\n",
    "# latent graph\n",
    "dag.add_edges({0}, 1)\n",
    "dag.add_edges({0}, 2)\n",
    "# dag.add_edges({1}, 2)\n",
    "\n",
    "# give it to sample data from dirichlet\n",
    "states = [5,5,5]\n",
    "data = sample_data_from_dag_dirichlet(dag, 50000, states=states)\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# run it.\n",
    "ci_test = CondIndepCMI(data, threshold=0.001)\n",
    "\n",
    "\n",
    "entropicpc = LearnStructPC(nodes_set = nodes_set, ci_test = ci_test, entropic = True, F = False, data = data, states = states)\n",
    "\n",
    "\n",
    "\n",
    "entropicpc.learn_structure()\n",
    "\n",
    "# to compute SHD, both graphs need to be CPDAG\n",
    "dag.convert_to_cpdag(pdag_copy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
